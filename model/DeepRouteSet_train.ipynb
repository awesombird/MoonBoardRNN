{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepRouteSet\n",
    "DeepRouteSet uses LSTM to generate new moonboard problem. \n",
    "It is modified from the homework \"Improvise a Jazz Solo with an LSTM Network\" of Coursera course \"Sequence Model\".\n",
    "Yi-Shiou Duh(Allenduh@stanford.edu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m to_categorical\n\u001b[1;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moptimizers\u001b[39;00m \u001b[39mimport\u001b[39;00m Adam\n\u001b[0;32m---> 14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmodel\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mDeepRouteSet_helper\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'model'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.backend import argmax\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "from tensorflow.keras.layers import Dense, Activation, Input, LSTM, Reshape, Lambda, RepeatVector\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from DeepRouteSet_helper import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = Path().cwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: all data preparation should be moved out of here to preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Data preparation\n",
    "\n",
    "We want more moonboard problems, and high-quality, hard moonboard problem is especially not enough. Thanksfully, with LSTM, we can generate unlimited problems. By carefully choosing the source of author and 3 stars high quality problems, we can generate new moonboard problem of better quality.\n",
    "\n",
    "\n",
    "### 1.1 - Dataset\n",
    "\n",
    "We will train our RNN using a sequence of moves. We already preprocessed moonboard problems from a set of holds (e.g., [8A, 11B, 13C, ...]) to the list of holds with designated hand operation (e.g., [8A-LH, 11B-RH...]). Each point will be assigned to a moonboard \"value\". After training, RNN will then be able to generate new move sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 - Define Path to read handString_seq\n",
    "\n",
    "5 files: Handsequence (benchmark YN/ grade YN) and raw data with URL: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_handString_seq_path = cwd.parent / 'preprocessing' / 'benchmark_handString_seq_X'\n",
    "benchmarkNoGrade_handString_seq_path = cwd.parent / 'preprocessing' / 'benchmarkNoGrade_handString_seq_X'\n",
    "nonbenchmark_handString_seq_path = cwd.parent / 'preprocessing' / 'nonbenchmark_handString_seq_X'\n",
    "nonbenchmarkNoGrade_handString_seq_path = cwd.parent / 'preprocessing' / 'nonbenchmarkNoGrade_handString_seq_X'\n",
    "\n",
    "url_data_path = cwd.parent / 'raw_data' / 'moonGen_scrape_2016_cp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(benchmark_handString_seq_path, 'rb') as f:\n",
    "    benchmark_handString_seq = pickle.load(f)\n",
    "with open(benchmarkNoGrade_handString_seq_path, 'rb') as f:\n",
    "    benchmarkNoGrade_handString_seq = pickle.load(f)\n",
    "with open(nonbenchmark_handString_seq_path, 'rb') as f:\n",
    "    nonbenchmark_handString_seq = pickle.load(f)\n",
    "with open(nonbenchmarkNoGrade_handString_seq_path, 'rb') as f:\n",
    "    nonbenchmarkNoGrade_handString_seq = pickle.load(f)   \n",
    "with open(url_data_path, 'rb') as f:\n",
    "    MoonBoard_2016_withurl = pickle.load(f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B4-LH', 'D6-RH', 'F9-LH', 'G11-RH', 'C15-LH', 'H18-RH']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of input sequence\n",
    "benchmark_handString_seq[\"189344\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'problem_name': 'JUST TRAINING',\n",
       " 'info': ['Alex Biale',\n",
       "  '195 climbers have repeated this problem',\n",
       "  '7C+ (User grade 8A)',\n",
       "  'Feet follow hands',\n",
       "  ''],\n",
       " 'url': 'https://moonboard.com/Problems/View/189344/just-training',\n",
       " 'num_empty': 0,\n",
       " 'num_stars': 3,\n",
       " 'moves': [{'Id': 1879483,\n",
       "   'Description': 'B4',\n",
       "   'IsStart': True,\n",
       "   'IsEnd': False},\n",
       "  {'Id': 1879484, 'Description': 'C15', 'IsStart': False, 'IsEnd': False},\n",
       "  {'Id': 1879485, 'Description': 'D6', 'IsStart': True, 'IsEnd': False},\n",
       "  {'Id': 1879486, 'Description': 'F9', 'IsStart': False, 'IsEnd': False},\n",
       "  {'Id': 1879487, 'Description': 'G11', 'IsStart': False, 'IsEnd': False},\n",
       "  {'Id': 1879488, 'Description': 'H18', 'IsStart': False, 'IsEnd': True}],\n",
       " 'grade': '7C+',\n",
       " 'UserGrade': '8A',\n",
       " 'isBenchmark': True,\n",
       " 'repeats': 195,\n",
       " 'ProblemType': 'Crimp',\n",
       " 'IsMaster': False,\n",
       " 'setter': {'Id': '7A90397F-74F9-4858-846B-D3CA9E4F70FF',\n",
       "  'Nickname': 'Alex Biale',\n",
       "  'Firstname': 'Alex',\n",
       "  'Lastname': 'Biale',\n",
       "  'City': 'Boulder',\n",
       "  'Country': 'CO',\n",
       "  'ProfileImageUrl': '/Content/Account/Images/default-profile.png?637231526210174359',\n",
       "  'CanShareData': True}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# raw data URL file looks like\n",
    "MoonBoard_2016_withurl[\"189344\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 - Pick good moonboard problem \n",
    "\n",
    "Only pick the problem by choosing good setter and pick 3 starts problems. Store in the goodKeyList\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Collect Name of all setters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All setter with non error name\n",
    "setterList = []\n",
    "countNumOfErrorUsername = 0\n",
    "for key in MoonBoard_2016_withurl.keys():\n",
    "    try:\n",
    "        setterList.append(MoonBoard_2016_withurl[key]['setter']['Nickname'])\n",
    "    except:\n",
    "        countNumOfErrorUsername += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Count how many problems each setter has set and add to dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Counter(setterList)    \n",
    " \n",
    "# TODO: reverse true? this means we are iterating in increasing number of problems - would be faster to do it in decreasing order\n",
    "setterDict = {k: v for k, v in sorted(Counter(setterList).items(), key=lambda item: item[1], reverse = True)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Add experienced setter and Benchmark setter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total good setter:  425\n"
     ]
    }
   ],
   "source": [
    "# add setter with 50+ experience or Benchmark setter\n",
    "goodSetterName = []\n",
    "for key in setterDict.keys():\n",
    "    if setterDict[key] > 50:\n",
    "        goodSetterName.append(key)\n",
    "for key in MoonBoard_2016_withurl.keys(): \n",
    "    try:\n",
    "        if MoonBoard_2016_withurl[key]['isBenchmark'] == True:\n",
    "            goodSetterName.append(MoonBoard_2016_withurl[key]['setter']['Nickname'])\n",
    "    except:\n",
    "        pass\n",
    "print(\"Total good setter: \", len(goodSetterName))     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Define a goodProblemKeyList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total amount of good problems:  19842\n"
     ]
    }
   ],
   "source": [
    "# Pick Extended BenchMark, high repeat number, author make many problem, high rate\n",
    "count = 0\n",
    "goodProblemKeyList = []\n",
    "for key in MoonBoard_2016_withurl.keys():  \n",
    "    try:\n",
    "        if MoonBoard_2016_withurl[key][isBenchmark] == True:\n",
    "            goodSetterName.append(MoonBoard_2016_withurl[key]['setter']['Nickname'])   \n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    \n",
    "for key in MoonBoard_2016_withurl.keys():  \n",
    "    try:\n",
    "        if MoonBoard_2016_withurl[key]['setter']['Nickname'] in goodSetterName:\n",
    "            goodProblemKeyList.append(key)\n",
    "            count = count + 1\n",
    "        if MoonBoard_2016_withurl[key]['isBenchmark'] == True:\n",
    "            goodProblemKeyList.append(key)\n",
    "            count = count + 1\n",
    "        if MoonBoard_2016_withurl[key]['repeats'] > 50:\n",
    "            goodProblemKeyList.append(key)\n",
    "            count = count + 1\n",
    "        if MoonBoard_2016_withurl[key]['num_stars'] == 3:\n",
    "            goodProblemKeyList.append(key) \n",
    "            count = count + 1\n",
    "    except:\n",
    "        pass\n",
    "print (\"Total amount of good problems: \", count)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 - Grade filter\n",
    "\n",
    "We will seperate problems into 3 difficulty range: Hard (V8 and up) medium (V6 to V8) and easy (V4 V5). So that our training set has the similar difficulty. This will help the problems generated to be more consistant in the difficulty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: model is split into 3 different versions for different grade ranges, reduces training data size\n",
    "# TODO: it could be better to add the grade as a feature, which would allow the model to learn connection between grade and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of easy problems: 9839\n",
      "num of medium problems: 8267\n",
      "num of hard problems: 4639\n"
     ]
    }
   ],
   "source": [
    "easyProblemKeyList = []\n",
    "mediumProblemKeyList = []\n",
    "hardProblemKeyList = []\n",
    "for key in goodProblemKeyList:\n",
    "#     TODO: this results in only graded problems being used in training\n",
    "    if MoonBoard_2016_withurl[key]['grade'] in [\"6B+\", \"6C\", \"6C+\"]: # V4 V5\n",
    "        easyProblemKeyList.append(key)\n",
    "    if MoonBoard_2016_withurl[key]['grade'] in [\"7A\", \"7A+\", \"7B\", \"7B+\"]: # V6 7 8\n",
    "        mediumProblemKeyList.append(key)\n",
    "    if MoonBoard_2016_withurl[key]['grade'] in [\"7B\", \"7B+\", \"7C\", \"7C+\", \"8A\", \"8A+\", \"8B\"]: # V8 9 10 11 12 13\n",
    "        hardProblemKeyList.append(key)        \n",
    "print(\"num of easy problems:\", len(easyProblemKeyList)) \n",
    "print(\"num of medium problems:\", len(mediumProblemKeyList))\n",
    "print(\"num of hard problems:\", len(hardProblemKeyList))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 - Collect good and proper grade range into handString training set\n",
    "Collect into handStringList as input of RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Define handStringList and add benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of benchmark training example:  358\n"
     ]
    }
   ],
   "source": [
    "# ensemble to a StringList\n",
    "print(\"Number of benchmark training example: \", len(benchmark_handString_seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now add more problems and add more Benchmark problem to emphasize Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Select a proper grade level "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "handStringList = collectHandStringIntoList(mediumProblemKeyList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of total training example (filter):  6681\n"
     ]
    }
   ],
   "source": [
    "# Total training sample\n",
    "numOfTrainingSample = len(handStringList)\n",
    "print(\"Number of total training example (filter): \", numOfTrainingSample)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 - construct total reservoir of all avalible holds\n",
    "Create two dictionaries: \n",
    "* holdStr_to_holdIx: \"J5-LH\" has index =  127\n",
    "* holdIx_to_holdStr: index 277 is hold B15-RH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* You can reload the dictionaries here (Skipe the later cells) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(cwd.parent / \"raw_data\" / \"holdStr_to_holdIx\", 'rb') as f:\n",
    "    holdStr_to_holdIx = pickle.load(f)\n",
    "with open(cwd.parent / \"raw_data\" / \"holdIx_to_holdStr\", 'rb') as f:\n",
    "    holdIx_to_holdStr = pickle.load(f)  \n",
    "numOfPossibleHolds = 277"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Code to regenerate maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: rewrite, very slow\n",
    "# # Merge all string to big list to know how many string to consider\n",
    "# holdsReservoir = [] \n",
    "# for i in range(len(handStringList)):\n",
    "#     holdsReservoir = holdsReservoir + handStringList[i]\n",
    "# #holdsReservoir = sorted(holdsReservoir)  It will be great to sort the num of String from bottom to top.  \n",
    "# holdsReservoir = list(set(holdsReservoir)) # Delete repetitive string\n",
    "# print('Total holds avalible (include L / R): ', len(holdsReservoir))\n",
    "# numOfPossibleHolds = len(holdsReservoir)\n",
    "\n",
    "# # Build a dictionary convert String \"J5-LH\" to index\n",
    "# holdStr_to_holdIx = {}\n",
    "# holdStr_to_holdIx[\"End\"] = 0  # End hold\n",
    "# for i in range(len(holdsReservoir)):\n",
    "#     holdStr_to_holdIx[holdsReservoir[i]] = i + 1\n",
    "# print('For example, \"J5-LH\" has index = ', holdStr_to_holdIx[\"J5-LH\"])   \n",
    "\n",
    "# holdIx_to_holdStr = {v: k for k, v in holdStr_to_holdIx.items()}\n",
    "# print('Reverse dictionary: index 23 is hold', holdIx_to_holdStr[23])  \n",
    "\n",
    "# save_pickle(holdStr_to_holdIx, cwd / 'holdStr_to_holdIx')\n",
    "# save_pickle(holdIx_to_holdStr, cwd / 'holdIx_to_holdStr')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 - Prepare RNN's inputXY \n",
    "Use loadSeqXYFromString to shift Y from X by 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training examples: 6681\n",
      "Tx (length of sequence): 12\n",
      "total # of unique values: 278\n",
      "shape of X: (6681, 12, 278)\n",
      "Shape of Y: (12, 6681, 278)\n"
     ]
    }
   ],
   "source": [
    "X, Y, n_values = loadSeqXYFromString(handStringList, holdStr_to_holdIx, m = numOfTrainingSample, maxNumOfHands = 12, numOfPossibleHolds = numOfPossibleHolds)\n",
    "print('number of training examples:', X.shape[0])\n",
    "print('Tx (length of sequence):', X.shape[1])\n",
    "print('total # of unique values:', n_values)\n",
    "print('shape of X:', X.shape)\n",
    "print('Shape of Y:', Y.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `X`: This is an `(m, Tx, n_values)` dimensional array. \n",
    "    - We have m training examples, each of which has `T_x=12` holds (if < 12 remainder are `\"End\"`/`0`). \n",
    "    - At each move step, the input is one of n_values different possible values, represented as a one-hot vector. \n",
    "        - For example, `X[i,t,:]` is a one-hot vector representing the hold at sequence position `t`. \n",
    "\n",
    "- `Y`: a `(Ty, m, n_values)` dimensional array\n",
    "    - This is essentially the same as `X`, but shifted one step to the left (to the previous move). \n",
    "    - Notice that the data in `Y` is **permuted** to be dimension `(Ty, m, n_values)`, where `Ty = Tx`. This format makes it more convenient to feed into the LSTM later.\n",
    "    - Similar to the music generator, we're using the previous values to predict the next value.\n",
    "        - So our sequence model will try to predict $y^{\\langle t \\rangle}$ given $x^{\\langle 1\\rangle}, \\ldots, x^{\\langle t \\rangle}$. \n",
    "\n",
    "- `n_values`: The number of unique values in this dataset. This should be `n_values`. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8 - Overview of our model\n",
    "\n",
    "* $X = (x^{\\langle 1 \\rangle}, x^{\\langle 2 \\rangle}, \\cdots, x^{\\langle T_x \\rangle})$ is a window of size $T_x$ scanned over the climbing moves. \n",
    "* Each $x^{\\langle t \\rangle}$ is an index corresponding to a value.\n",
    "* $\\hat{y}^{t}$ is the prediction for the next value.\n",
    "* We will be training the model on random 12 values padded  with `\"End\"` if the climbing problem end before. \n",
    "    - We are setting each of the snippets to have the same length $T_x = 12$ to make vectorization easier."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of parts 2 and 3 (If you already trained the model, skip to Prediction part 4)\n",
    "\n",
    "\n",
    "- We're going to train a model that predicts the next hold in a style that is similar to the climbing problems that it's trained on.  The training is contained in the weights and biases of the model. \n",
    "- In Part 3, we're then going to use those weights and biases in a new model which predicts a series of holds, using the previous hold to predict the next hold. \n",
    "- The weights and biases are transferred to the new model using 'global shared layers' described below\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Building the model (If you already trained the model, skip to Prediction part)\n",
    "\n",
    "* Build and train a model that will learn climbing moves patterns. \n",
    "* The model takes input X of shape $(m, T_x, n_values)$ and labels Y of shape $(T_y, m, n_values)$. \n",
    "* We will use an LSTM with hidden states that have $n_{a} = 64$ dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of dimensions for the hidden state of each LSTM cell.\n",
    "n_a = 64 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Sequence generation uses a for-loop\n",
    "* If you're building an RNN where, at test time, the entire input sequence $x^{\\langle 1 \\rangle}, x^{\\langle 2 \\rangle}, \\ldots, x^{\\langle T_x \\rangle}$ is given in advance, then Keras has simple built-in functions to build the model. \n",
    "* However, for **sequence generation, at test time we don't know all the values of $x^{\\langle t\\rangle}$ in advance**.\n",
    "* Instead we generate them one at a time using $x^{\\langle t\\rangle} = y^{\\langle t-1 \\rangle}$. \n",
    "    * The input at time `t` is the prediction at the previous time step `t-1`.\n",
    "\n",
    "#### Shareable weights\n",
    "* The function `routeSetmodel()` will call the LSTM layer $T_x$ times using a for-loop.\n",
    "* It is important that all $T_x$ copies have the same weights. \n",
    "    - The $T_x$ steps should have shared weights that aren't re-initialized.\n",
    "* Referencing a globally defined shared layer will utilize the same layer-object instance at each time step.\n",
    "* Benefits of using shareable weights:\n",
    "    - reduced computation and space requirements\n",
    "    - theoretically has the ability to memorize long-term dependencies\n",
    "    - we may however learn more complex patterns if we don't share weights which is unlikely to require much more computation as we only generate 12 holds \n",
    "\n",
    "\n",
    "The key steps for implementing layers with shareable weights in Keras are: \n",
    "1. Define the layer objects (we will use global variables for this).\n",
    "2. Call these objects when propagating the input.\n",
    "\n",
    "#### 3 types of global variables layers:\n",
    "- [Reshape()](https://keras.io/layers/core/#reshape): Reshapes an output to a certain shape.\n",
    "- [LSTM()](https://keras.io/layers/recurrent/#lstm): Long Short-Term Memory layer\n",
    "- [Dense()](https://keras.io/layers/core/#dense): A regular fully-connected neural network layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: testing and reconsideration of shared weights\n",
    "reshapor = Reshape((1, n_values))\n",
    "LSTM_cell = LSTM(n_a, return_state = True)        \n",
    "densor = Dense(n_values, activation='softmax')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inputs \n",
    "* The `Input()` layer is used for defining the input `X` as well as the initial hidden state 'a0' and cell state `c0`.\n",
    "* The `shape` parameter takes a tuple that does not include the batch dimension (`m`).\n",
    "```Python\n",
    "X = Input(shape=(Tx, n_values)) # X has 3 dimensions and not 2: (m, Tx, n_values)\n",
    "```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Outputs \n",
    "- Create an empty list `outputs` to save the outputs of the LSTM Cell at every time step."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Loop through time steps\n",
    "* Loop for $t \\in 1, \\ldots, T_x$:\n",
    "\n",
    "#### 2A. Select the `t` time-step vector from `X`.\n",
    "* `X` has the shape `(m, Tx, n_values)`.\n",
    "* The shape of the `t` selection should be `(n_values,)`.\n",
    "\n",
    "#### 2B. Reshape `x` to be `(1,n_values)`.\n",
    "* Use the `reshapor()` layer.  It is a function that takes the previous layer as its input argument.\n",
    "\n",
    "#### 2C. Run `x` through one step of LSTM_cell.\n",
    "* Initialize the `LSTM_cell` with the previous step's hidden state `a` and cell state `c`. \n",
    "* Use the following formatting:\n",
    "```python\n",
    "next_hidden_state, _, next_cell_state = LSTM_cell(inputs=input_x, initial_state=[previous_hidden_state, previous_cell_state])\n",
    "```\n",
    "\n",
    "#### 2D. Dense layer\n",
    "* Propagate the LSTM's hidden state through a dense+softmax layer using `densor`. \n",
    "    \n",
    "#### 2E. Append output\n",
    "* Append the output to the list of `outputs`.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: After the loop, create the model\n",
    "* Use the Keras `Model` object to create a model.\n",
    "* specify the inputs and outputs:\n",
    "```Python\n",
    "model = Model(inputs=[input_x, initial_hidden_state, initial_cell_state], outputs=the_outputs)\n",
    "```\n",
    "* Choose the appropriate variables for the input tensor, hidden state, cell state, and output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deepRouteSet(Tx, n_a, n_values):\n",
    "    \"\"\"\n",
    "    Training model for route generation we reuse these  weights with a different network in order to generate routes\n",
    "    \n",
    "    Arguments:\n",
    "    Tx -- length of the climbing route (padded by \"End\" up to this length)\n",
    "    n_a -- the number of activations used by the LSTM cells in our model\n",
    "    n_values -- number of unique values in the climbing move data \n",
    "    \n",
    "    Returns:\n",
    "    model -- a keras instance model with n_a activations\n",
    "    \"\"\"\n",
    "    # TODO: think this needs to be reverted to use layers passed as inputs so weights can be used later without loading\n",
    "    reshapor = Reshape((1, n_values))\n",
    "    LSTM_cell = LSTM(n_a, return_state = True)\n",
    "    densor = Dense(n_values, activation='softmax')\n",
    "    \n",
    "    # Define the input layer and specify the shape\n",
    "    X = Input(shape=(Tx, n_values))\n",
    "    \n",
    "    # Define the initial hidden state a0 and initial cell state c0\n",
    "    # using `Input`\n",
    "    a0 = Input(shape=(n_a,), name='a0')\n",
    "    c0 = Input(shape=(n_a,), name='c0')\n",
    "    a = a0\n",
    "    c = c0\n",
    "    \n",
    "    # Step 1: Create empty list to append the outputs while you iterate\n",
    "    outputs = []\n",
    "    \n",
    "    # Step 2: Loop\n",
    "    for t in range(Tx):      \n",
    "        # Step 2.A: select the \"t\"th time step vector from X. \n",
    "        x = Lambda(lambda z: z[:, t, :])(X)   \n",
    "        # Step 2.B: Use reshapor to reshape x to be (1, n_values)\n",
    "        x = reshapor(x)  # from (?, n_values) to (?, 1, n_values)\n",
    "        # Step 2.C: Perform one step of the LSTM_cell\n",
    "        a, _, c = LSTM_cell(inputs = x, initial_state = [a, c])\n",
    "        # Step 2.D: Apply densor to the hidden state output of LSTM_Cell\n",
    "        out = densor(a)\n",
    "        # Step 2.E: add the output to \"outputs\"\n",
    "        outputs.append(out)\n",
    "        \n",
    "    # Step 3: Create model instance\n",
    "    model = Model(inputs = [X, a0, c0], outputs = outputs)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the model object\n",
    "* Run the following cell to define your model. \n",
    "* We will use `Tx=12`, `n_a=64` (the dimension of the LSTM activations), and `n_values=n_values`. \n",
    "* This cell may take a few seconds to run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = deepRouteSet(Tx = 12 , n_a = 64, n_values = n_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check model\n",
    "# model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile the model for training\n",
    "- optimizer: Adam\n",
    "- loss: categorical cross-entropy (for multi-class classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(learning_rate=0.01, beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize hidden state and cell state\n",
    "Finally, let's initialize `a0` and `c0` for the LSTM's initial state to be zero. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m = numOfTrainingSample\n",
    "a0 = np.zeros((m, n_a))\n",
    "c0 = np.zeros((m, n_a))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model\n",
    "* Lets now fit the model! \n",
    "* We will turn `Y` into a list, since the cost function expects `Y` to be provided in this format \n",
    "    - `list(Y)` is a list with `Tx` items, where each of the list items is of shape (numOfTrainingSample,n_values). \n",
    "    - Lets train for 100 epochs. This will take a few minutes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load weights instead of retraining\n",
    "model.load_weights(cwd.parent / 'model' / 'DeepRouteSet_weights_medium.h5')\n",
    "# model.fit([X, a0, c0], list(Y), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add tesorflow callback, training checkpoints and early stopping"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Generating moonboard problem\n",
    "\n",
    "We already have trained model which has learned the patterns of climbing move and route set spirit. Lets now use this model to synthesize new route.\n",
    "Importantly we reuse the `LSTM_cell` and `densor` layers from the model we trained.\n",
    "\n",
    "### 3.1 - The inference model\n",
    "\n",
    "At each step of sampling:\n",
    "* Take as input the activation '`a`' and cell state '`c`' from the previous state of the LSTM.\n",
    "* Forward propagate by one step.\n",
    "* Get a new output activation as well as cell state. \n",
    "* The new activation '`a`' can then be used to generate the output using the fully connected layer, `densor`. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Sample x to be the one-hot version of '`out`'. \n",
    "* This allows you to pass it to the next LSTM's step.  \n",
    "* use the add_one function inside of the Lambda function\n",
    "```py\n",
    "result = Lambda(add_one)(input_var)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deepRouteSetPred(LSTM_cell, densor, n_values = n_values, n_a = 64, Ty = 12):\n",
    "    \"\"\"\n",
    "    Uses the trained \"LSTM_cell\" and \"densor\" from model() to generate a sequence of values.\n",
    "    \n",
    "    Arguments:\n",
    "    LSTM_cell -- the trained \"LSTM_cell\" from model(), Keras layer object\n",
    "    densor -- the trained \"densor\" from model(), Keras layer object\n",
    "    n_values -- integer, number of unique values\n",
    "    n_a -- number of units in the LSTM_cell\n",
    "    Ty -- integer, number of time steps to generate\n",
    "    \n",
    "    Returns:\n",
    "    inference_model -- Keras model instance\n",
    "    \"\"\"\n",
    "    # TODO: replace this lambda function with actual layers\n",
    "    def one_hot(x):\n",
    "        x = argmax(x)\n",
    "        x = tf.one_hot(x, n_values) \n",
    "        x = RepeatVector(1)(x)\n",
    "        return x\n",
    "    \n",
    "    # Define the input of your model with a shape \n",
    "    x0 = Input(shape=(1, n_values))\n",
    "    \n",
    "    # Define s0, initial hidden state for the decoder LSTM\n",
    "    a0 = Input(shape=(n_a,), name='a0')\n",
    "    c0 = Input(shape=(n_a,), name='c0')\n",
    "    a = a0\n",
    "    c = c0\n",
    "    x = x0\n",
    "\n",
    "    # Step 1: Create an empty list of \"outputs\" to later store your predicted values\n",
    "    outputs = []\n",
    "    \n",
    "    # Step 2: Loop over Ty and generate a value at every time step\n",
    "    for t in range(Ty):\n",
    "        \n",
    "        # Step 2.A: Perform one step of LSTM_cell\n",
    "        # TODO: this is using the same LSTM_cell for all time steps?\n",
    "        a, _, c = LSTM_cell(x, initial_state=[a, c])\n",
    "        \n",
    "        # Step 2.B: Apply Dense layer to the hidden state output of the LSTM_cell\n",
    "        out = densor(a)\n",
    "\n",
    "        # Step 2.C: Append the prediction \"out\" to \"outputs\". out.shape = (None, n_values)\n",
    "        outputs.append(out)\n",
    "        \n",
    "        # Step 2.D: \n",
    "        # Select the next value according to \"out\",\n",
    "        # Set \"x\" to be the one-hot representation of the selected value\n",
    "        # See instructions above.\n",
    "        x = Lambda(one_hot)(out)\n",
    "        \n",
    "        \n",
    "    # Step 3: Create model instance with the correct \"inputs\" and \"outputs\"\n",
    "    inference_model = Model(inputs = [x0, a0, c0], outputs = outputs)\n",
    "        \n",
    "    return inference_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_model = deepRouteSetPred(LSTM_cell, densor, n_values = n_values, n_a = n_a, Ty = 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check the inference model\n",
    "# inference_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load weights learnt by training model\n",
    "inference_model.load_weights(cwd / 'DeepRouteSet_medium.h5')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c:\\Users\\valsp\\source\\repos\\MoonBoardRNN\\model\\DeepRouteSet\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c:\\Users\\valsp\\source\\repos\\MoonBoardRNN\\model\\DeepRouteSet\\assets\n"
     ]
    }
   ],
   "source": [
    "# create DeepRouteSet directory if it doesn't exist\n",
    "model_dir = cwd / 'DeepRouteSet'\n",
    "if not model_dir.exists():\n",
    "    model_dir.mkdir()\n",
    "\n",
    "# save the complete inference model architecture and weights\n",
    "inference_model.save(model_dir, include_optimizer=False)"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "nlp-sequence-models",
   "graded_item_id": "EG0F7",
   "launcher_item_id": "cxJXc"
  },
  "kernelspec": {
   "display_name": "moonboardrnn-hW4ed92X-py3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "8d257aaba97b534713eafa91609455863fac8d9daac4f63bd3ddeb8aecd89a62"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
