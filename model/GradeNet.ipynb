{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "import keras\n",
    "import copy\n",
    "import tensorflow as tf\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation, Dense\n",
    "from keras.layers import Flatten, LSTM, Masking\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from sklearn import metrics\n",
    "from model_helper import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "parent_wd = cwd.replace('\\\\', '/').replace('/model', '')\n",
    "training_set_path = parent_wd + '/preprocessing/training_seq_n_12_rmrp0'\n",
    "dev_set_path = parent_wd + '/preprocessing/dev_seq_n_12_rmrp0'\n",
    "test_set_path = parent_wd + '/preprocessing/test_seq_n_12_rmrp0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(training_set_path, 'rb') as f:\n",
    "    training_set = pickle.load(f)\n",
    "with open(dev_set_path, 'rb') as f:\n",
    "    dev_set = pickle.load(f)\n",
    "with open(test_set_path, 'rb') as f:\n",
    "    test_set = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(training_set['X'])\n",
    "Y_train = np.array(training_set['Y'])\n",
    "X_dev = np.array(dev_set['X'])\n",
    "Y_dev = np.array(dev_set['Y'])\n",
    "X_test = np.array(test_set['X'])\n",
    "Y_test = np.array(test_set['Y'])\n",
    "\n",
    "print(Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define GradeNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)\n",
    "inputs = Input(shape = (12, 22))\n",
    "mask = Masking(mask_value = 0.).compute_mask(inputs)\n",
    "lstm0 = LSTM(20, activation='tanh', input_shape=(12, 22), kernel_initializer='glorot_normal', return_sequences = 'True')(\n",
    "    inputs, mask = mask)\n",
    "dense1 = Dense(100, activation='relu', kernel_initializer='glorot_normal')(lstm0)\n",
    "dense2 = Dense(80, activation='relu', kernel_initializer='glorot_normal')(dense1)\n",
    "dense3 = Dense(75, activation='relu', kernel_initializer='glorot_normal')(dense2)\n",
    "dense4 = Dense(50, activation='relu', kernel_initializer='glorot_normal')(dense3)\n",
    "dense5 = Dense(20, activation='relu', kernel_initializer='glorot_normal')(dense4)\n",
    "dense6 = Dense(10, activation='relu', kernel_initializer='glorot_normal')(dense5)\n",
    "flat = Flatten()(dense6)\n",
    "softmax2 = Dense(10, activation='softmax', name = 'softmax2')(flat)\n",
    "lstm1 = LSTM(20, activation='tanh', kernel_initializer='glorot_normal', return_sequences = True)(dense6)\n",
    "lstm2 = LSTM(20, activation='tanh', kernel_initializer='glorot_normal')(lstm1)\n",
    "dense7 = Dense(15, activation='relu', kernel_initializer='glorot_normal')(lstm2)\n",
    "dense8 = Dense(15, activation='relu', kernel_initializer='glorot_normal')(dense7)\n",
    "softmax3 = Dense(10, activation='softmax', name = 'softmax2')(dense8)\n",
    "\n",
    "def custom_loss(layer):\n",
    "    def loss(y_true,y_pred):\n",
    "        loss1 = K.sparse_categorical_crossentropy(y_true, y_pred)\n",
    "        loss2 = K.sparse_categorical_crossentropy(y_true, layer)\n",
    "        return K.mean(loss1 + loss2, axis=-1)\n",
    "    return loss\n",
    "\n",
    "GradeNet = Model(inputs=[inputs], outputs=[softmax3])\n",
    "GradeNet.compile(optimizer='adam', \n",
    "                loss='sparse_categorical_crossentropy' ,#loss=custom_loss(softmax2), -loss func seems to break it, idk how to fix it\n",
    "                metrics=['sparse_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-1. Training of GradeNet\n",
    "### To load pretrained weights, please skip to 2-2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_GradeNet_all = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    history_GradeNet = GradeNet.fit(X_train, Y_train, epochs=10, batch_size=256, validation_data = (X_dev, Y_dev), \n",
    "                                class_weight = {0:1, 1:1, 2:2, 3: 2, 4: 1, 5: 4, 6:2, 7: 4, 8: 8, 9: 8})\n",
    "    history_GradeNet_all.append(history_GradeNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Change weights\n",
    "for i in range(10):\n",
    "    history_GradeNet = GradeNet.fit(X_train, Y_train, epochs=10, batch_size=256, validation_data = (X_dev, Y_dev), \n",
    "                                class_weight = {0:1, 1:1, 2:2, 3: 4, 4: 1, 5: 4, 6: 8, 7: 8, 8: 8, 9: 8})\n",
    "    history_GradeNet_all.append(history_GradeNet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GradeNet_history_package = plot_history(history_GradeNet_all, 'GradeNet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving trained results\n",
    "save_pickle(GradeNet_history_package, 'GradeNet_train_history')\n",
    "GradeNet.save_weights(\"GradeNet.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-2. Loading pretrained GradeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model weight\n",
    "GradeNet.load_weights(parent_wd + '/model/GradeNet.h5')\n",
    "\n",
    "# load training history\n",
    "history_path = parent_wd + '/model/GradeNet_train_history'\n",
    "with open(history_path, 'rb') as f:\n",
    "    GradeNet_history_package = pickle.load(f)\n",
    "\n",
    "plot_history_package(GradeNet_history_package, 'GradeNet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analyze GradeNet Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(Y_train, GradeNet.predict(X_train).argmax(axis=1), title = 'Confusion matrix of GradeNet(Training set)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(Y_dev, GradeNet.predict(X_dev).argmax(axis=1), title = 'Confusion matrix of GradeNet(Dev set)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(Y_test, GradeNet.predict(X_test).argmax(axis=1), title = 'Confusion matrix of GradeNet(Test set)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F1_train = metrics.f1_score(Y_train, GradeNet.predict(X_train).argmax(axis=1), average = 'macro')\n",
    "print(F1_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F1_dev = metrics.f1_score(Y_dev, GradeNet.predict(X_dev).argmax(axis=1), average = 'macro')\n",
    "print(F1_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F1_test = metrics.f1_score(Y_test, GradeNet.predict(X_test).argmax(axis=1), average = 'macro')\n",
    "print(F1_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy and Rough accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_train = compute_accuracy(Y_train, GradeNet.predict(X_train).argmax(axis=1))\n",
    "print(\"Exactly accuracy rate of training set = %s\" %accuracy_train[0])\n",
    "print(\"+/-1 Accuracy rate of training set= %s\" %accuracy_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_dev = compute_accuracy(Y_dev, GradeNet.predict(X_dev).argmax(axis=1))\n",
    "print(\"Exactly accuracy rate of dev set = %s\" %accuracy_dev[0])\n",
    "print(\"+/-1 Accuracy rate of dev set = %s\" %accuracy_dev[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_test = compute_accuracy(Y_test, GradeNet.predict(X_test).argmax(axis=1))\n",
    "print(\"Exactly accuracy rate of test set = %s\" %accuracy_test[0])\n",
    "print(\"+/-1 Accuracy rate of test set = %s\" %accuracy_test[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KL divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kl = tf.keras.losses.KLDivergence()\n",
    "\n",
    "kld_train = kl(tf.one_hot(Y_train.astype(int), depth = 10), GradeNet.predict(X_train)).numpy()\n",
    "print(kld_train)\n",
    "\n",
    "kld_dev = kl(tf.one_hot(Y_dev.astype(int), depth = 10), GradeNet.predict(X_dev)).numpy()\n",
    "print(kld_dev)\n",
    "\n",
    "kld_test = kl(tf.one_hot(Y_test.astype(int), depth = 10), GradeNet.predict(X_test)).numpy()\n",
    "print(kld_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean absolute error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAE\n",
    "mae_train = np.mean(np.abs(Y_train - GradeNet.predict(X_train).argmax(axis=1)))\n",
    "print(mae_train)\n",
    "\n",
    "mae_dev = np.mean(np.abs(Y_dev - GradeNet.predict(X_dev).argmax(axis=1)))\n",
    "print(mae_dev)\n",
    "\n",
    "mae_test = np.mean(np.abs(Y_test - GradeNet.predict(X_test).argmax(axis=1)))\n",
    "print(mae_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk_report_train = metrics.classification_report(\n",
    "    digits=4,\n",
    "    y_true=Y_train, \n",
    "    y_pred=GradeNet.predict(X_train).argmax(axis=1))\n",
    "print(sk_report_train)\n",
    "\n",
    "sk_report_dev = metrics.classification_report(\n",
    "    digits=4,\n",
    "    y_true=Y_dev, \n",
    "    y_pred=GradeNet.predict(X_dev).argmax(axis=1))\n",
    "print(sk_report_dev)\n",
    "\n",
    "sk_report_test = metrics.classification_report(\n",
    "    digits=4,\n",
    "    y_true=Y_test, \n",
    "    y_pred=GradeNet.predict(X_test).argmax(axis=1))\n",
    "print(sk_report_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUC_train = metrics.roc_auc_score(Y_train, GradeNet.predict(X_train), multi_class= 'ovr', average=\"macro\")\n",
    "print(AUC_train)\n",
    "\n",
    "AUC_dev = metrics.roc_auc_score(Y_dev, GradeNet.predict(X_dev), multi_class= 'ovr', average=\"macro\")\n",
    "print(AUC_dev)\n",
    "\n",
    "AUC_test = metrics.roc_auc_score(Y_test, GradeNet.predict(X_test), multi_class= 'ovr', average=\"macro\")\n",
    "print(AUC_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('moonboardrnn-pRsO0z7Q-py3.10')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "fa12d531b920e28e7fee8613a64d39c170ddd78b5a4f3da32a4a6856cd38bf1b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
